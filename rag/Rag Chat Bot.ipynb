{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78a6c9c-e1e8-4d96-bb92-100d81b74332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (25.3)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install RAG prototype dependencies\n",
    "# Run this cell in your Jupyter notebook. It may take 1-3 minutes.\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "\n",
    "# Core libs\n",
    "!{sys.executable} -m pip install \"langchain>=0.0.200\" langchain-openai langchain-neo4j neo4j neo4j-driver\n",
    "\n",
    "# Document loading and PDF parsing\n",
    "!{sys.executable} -m pip install pypdf langchain-io\n",
    "\n",
    "# Optional: community loaders (if needed)\n",
    "!{sys.executable} -m pip install langchain-community\n",
    "\n",
    "# Embeddings & FAISS (cpu)\n",
    "!{sys.executable} -m pip install openai faiss-cpu\n",
    "\n",
    "# Small convenience libs\n",
    "!{sys.executable} -m pip install python-dotenv\n",
    "\n",
    "print(\"Install commands finished. Now running quick import tests...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8b9f15e-f2f0-46c7-8870-8fab70aad3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports OK ✅ - environment ready for RAG prototype.\n"
     ]
    }
   ],
   "source": [
    "# Step 1b: Quick import test to confirm everything installed\n",
    "errors = {}\n",
    "try:\n",
    "    import langchain\n",
    "except Exception as e:\n",
    "    errors['langchain'] = str(e)\n",
    "try:\n",
    "    import neo4j\n",
    "except Exception as e:\n",
    "    errors['neo4j'] = str(e)\n",
    "try:\n",
    "    import pypdf\n",
    "except Exception as e:\n",
    "    errors['pypdf'] = str(e)\n",
    "try:\n",
    "    import openai\n",
    "except Exception as e:\n",
    "    errors['openai'] = str(e)\n",
    "try:\n",
    "    import faiss\n",
    "except Exception as e:\n",
    "    errors['faiss'] = str(e)\n",
    "\n",
    "if errors:\n",
    "    print(\"IMPORT ERRORS:\", errors)\n",
    "else:\n",
    "    print(\"All imports OK ✅ - environment ready for RAG prototype.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9093be9f-c3f0-4705-8357-1224023c3526",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\neo4j\\_sync\\work\\result.py:625: UserWarning: Expected a result with a single record, but found multiple.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB Info: {'name': 'Neo4j Kernel', 'versions': ['5.27-aura'], 'edition': 'enterprise'}\n",
      "TestNode count: 1\n",
      "Cleanup complete.\n",
      "\n",
      "Connected to Neo4j Aura successfully! ✅\n"
     ]
    }
   ],
   "source": [
    "# Step 4 — Test Neo4j Aura connection\n",
    "\n",
    "NEO4J_URI = \"neo4j+s://d333bb5f.databases.neo4j.io\"   # ← replace\n",
    "NEO4J_USER = \"neo4j\"                                  # ← replace if different\n",
    "NEO4J_PASSWORD = \"ULBfmPb5dsCbkEjzQpdfvGroJvnW28MFDXYdGC6p9m4\"                 # ← replace\n",
    "\n",
    "from neo4j import GraphDatabase, basic_auth\n",
    "import traceback\n",
    "\n",
    "driver = None\n",
    "try:\n",
    "    # IMPORTANT: removed encrypted=True for Aura\n",
    "    driver = GraphDatabase.driver(\n",
    "        NEO4J_URI,\n",
    "        auth=basic_auth(NEO4J_USER, NEO4J_PASSWORD)\n",
    "    )\n",
    "\n",
    "    with driver.session() as session:\n",
    "        info = session.run(\n",
    "            \"CALL dbms.components() YIELD name, versions, edition RETURN name, versions, edition\"\n",
    "        ).single()\n",
    "\n",
    "        print(\"DB Info:\", dict(info))\n",
    "\n",
    "        # Test writing\n",
    "        session.run(\"CREATE (t:TestNode {created: datetime()})\")\n",
    "        count = session.run(\"MATCH (t:TestNode) RETURN count(t) AS c\").single().get(\"c\")\n",
    "        print(\"TestNode count:\", count)\n",
    "\n",
    "        # Cleanup\n",
    "        session.run(\"MATCH (t:TestNode) DETACH DELETE t\")\n",
    "        print(\"Cleanup complete.\")\n",
    "\n",
    "    print(\"\\nConnected to Neo4j Aura successfully! ✅\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Connection failed\")\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    if driver:\n",
    "        driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c2488b8-6cf8-4952-9830-b953c787921c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in current directory:\n",
      " - .ipynb_checkpoints\n",
      " - Rag Chat Bot.ipynb\n",
      " - Web Dev using AI Course Content.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Files in current directory:\")\n",
    "for f in os.listdir('.'):\n",
    "    print(\" -\", f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3601e67-92b2-4e49-946b-1568ae94517b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .env\n"
     ]
    }
   ],
   "source": [
    "%%writefile .env\n",
    "OPENAI_API_KEY=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\n",
    "NEO4J_URI=neo4j+s://d333bb5f.databases.neo4j.io\n",
    "NEO4J_USER=neo4j\n",
    "NEO4J_PASSWORD=ULBfmPb5dsCbkEjzQpdfvGroJvnW28MFDXYdGC6p9m4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ee1e387-3eb9-4f1c-b9d7-403ae340b116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded OPENAI_API_KEY: True\n",
      "Neo4j URI: neo4j+s://d333bb5f.databases.neo4j.io\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "print(\"Loaded OPENAI_API_KEY:\", \"OPENAI_API_KEY\" in os.environ)\n",
    "print(\"Neo4j URI:\", os.environ.get(\"NEO4J_URI\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db54b671-9d6a-421a-a28b-a53bfdb61de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-text-splitters in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-text-splitters) (1.0.5)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.4.43)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.10.3)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (4.12.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.25.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.27.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.26.20)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-text-splitters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb523714-48ca-4fd0-9172-e4ec2787935f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF...\n",
      "Extracted 132 pages; total chars: 170517\n",
      "Split into 294 chunks\n",
      "Creating embeddings (OpenAI)...\n",
      "Created 294 embeddings (dim=1536)\n",
      "FAISS index saved.\n",
      "Writing chunks to Neo4j...\n",
      "Created/updated 294 nodes in Neo4j.\n",
      "Ingestion complete! ✅\n"
     ]
    }
   ],
   "source": [
    "# Ingest PDF -> chunks -> embeddings -> FAISS -> Neo4j\n",
    "import os, uuid, json, traceback\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from neo4j import GraphDatabase, basic_auth\n",
    "\n",
    "# config\n",
    "PDF_FILENAME = \"Web Dev using AI Course Content.pdf\"\n",
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 200\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "FAISS_DIR = \"faiss_store\"\n",
    "BATCH = 50\n",
    "\n",
    "# creds\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "NEO4J_URI = os.environ.get(\"NEO4J_URI\")\n",
    "NEO4J_USER = os.environ.get(\"NEO4J_USER\")\n",
    "NEO4J_PASSWORD = os.environ.get(\"NEO4J_PASSWORD\")\n",
    "\n",
    "if not (OPENAI_API_KEY and NEO4J_URI and NEO4J_USER and NEO4J_PASSWORD):\n",
    "    raise RuntimeError(\"Missing credentials. Run load_dotenv().\")\n",
    "\n",
    "# --- IMPORTS (updated for LC 0.1+) ---\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import faiss\n",
    "\n",
    "print(\"Loading PDF...\")\n",
    "loader = PyPDFLoader(PDF_FILENAME)\n",
    "pages = loader.load()\n",
    "full_text = \"\\n\\n\".join([p.page_content for p in pages])\n",
    "print(f\"Extracted {len(pages)} pages; total chars: {len(full_text)}\")\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP\n",
    ")\n",
    "chunks = splitter.split_text(full_text)\n",
    "print(f\"Split into {len(chunks)} chunks\")\n",
    "\n",
    "docs = []\n",
    "for i, txt in enumerate(chunks):\n",
    "    docs.append((txt, {\n",
    "        \"chunk_index\": i,\n",
    "        \"chunk_id\": str(uuid.uuid4()),\n",
    "        \"source_pdf\": PDF_FILENAME\n",
    "    }))\n",
    "\n",
    "print(\"Creating embeddings (OpenAI)...\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "emb = OpenAIEmbeddings(model=EMBEDDING_MODEL)\n",
    "vectors = emb.embed_documents([t for t,_ in docs])\n",
    "dim = len(vectors[0])\n",
    "print(f\"Created {len(vectors)} embeddings (dim={dim})\")\n",
    "\n",
    "# build FAISS\n",
    "xb = np.array(vectors).astype(\"float32\")\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(xb)\n",
    "\n",
    "os.makedirs(FAISS_DIR, exist_ok=True)\n",
    "faiss.write_index(index, os.path.join(FAISS_DIR, \"index.faiss\"))\n",
    "\n",
    "with open(os.path.join(FAISS_DIR, \"metadata.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump([m for _,m in docs], f, indent=2)\n",
    "\n",
    "print(\"FAISS index saved.\")\n",
    "\n",
    "# upload to Neo4j\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=basic_auth(NEO4J_USER, NEO4J_PASSWORD))\n",
    "created = 0\n",
    "\n",
    "print(\"Writing chunks to Neo4j...\")\n",
    "try:\n",
    "    with driver.session() as session:\n",
    "        for i in range(0, len(docs), BATCH):\n",
    "            batch = docs[i:i+BATCH]\n",
    "            tx = session.begin_transaction()\n",
    "            for txt, meta in batch:\n",
    "                tx.run(\"\"\"\n",
    "                    MERGE (c:DocChunk {chunk_id:$chunk_id})\n",
    "                    SET c.chunk_index = $chunk_index,\n",
    "                        c.source_pdf = $source_pdf,\n",
    "                        c.text_preview = $text_preview\n",
    "                \"\"\", \n",
    "                chunk_id=meta[\"chunk_id\"],\n",
    "                chunk_index=meta[\"chunk_index\"],\n",
    "                source_pdf=meta[\"source_pdf\"],\n",
    "                text_preview=txt[:2000])\n",
    "                created += 1\n",
    "            tx.commit()\n",
    "    print(f\"Created/updated {created} nodes in Neo4j.\")\n",
    "except Exception:\n",
    "    print(\"Neo4j write error:\")\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    driver.close()\n",
    "\n",
    "print(\"Ingestion complete! ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12fdb9c3-ac79-4491-b685-dad1369286d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask anything based on the course:\n",
      " what are the best ai codng assisstants or tools?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thinking...\n",
      "\n",
      "\n",
      "--- Answer ---\n",
      "\n",
      "Some popular AI coding assistants and tools include GitHub Copilot, Replit AI, and VS Code with AI extensions like GitHub Copilot, Codeium, and TabNine. These tools can suggest code, build entire projects, and provide real-time coding assistance to help you write code, solve problems, and guide you through challenges in your development projects.\n",
      "\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "# STEP 6c — Natural Chatbot Answer (no citations, smooth response)\n",
    "import os, json, numpy as np\n",
    "import faiss\n",
    "from neo4j import GraphDatabase, basic_auth\n",
    "from openai import OpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "FAISS_DIR = \"faiss_store\"\n",
    "TOP_K = 5\n",
    "EMBED_MODEL = \"text-embedding-3-small\"\n",
    "LLM_MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "# load FAISS + metadata\n",
    "index = faiss.read_index(os.path.join(FAISS_DIR, \"index.faiss\"))\n",
    "with open(os.path.join(FAISS_DIR, \"metadata.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "    metadata_list = json.load(f)\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# embedder\n",
    "emb = OpenAIEmbeddings(model=EMBED_MODEL)\n",
    "\n",
    "query = input(\"Ask anything based on the course:\\n\").strip()\n",
    "qvec = emb.embed_query(query)\n",
    "qvec_np = np.array(qvec).astype(\"float32\").reshape(1, -1)\n",
    "\n",
    "D, I = index.search(qvec_np, TOP_K)\n",
    "indices = I[0].tolist()\n",
    "\n",
    "# Fetch chunk text from Neo4j\n",
    "NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD = (\n",
    "    os.environ.get(\"NEO4J_URI\"),\n",
    "    os.environ.get(\"NEO4J_USER\"),\n",
    "    os.environ.get(\"NEO4J_PASSWORD\"),\n",
    ")\n",
    "\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=basic_auth(NEO4J_USER, NEO4J_PASSWORD))\n",
    "\n",
    "chunks = []\n",
    "with driver.session() as session:\n",
    "    for idx in indices:\n",
    "        meta = metadata_list[idx]\n",
    "        rec = session.run(\n",
    "            \"MATCH (c:DocChunk {chunk_index:$ci}) RETURN c.text_preview AS p\",\n",
    "            ci=meta[\"chunk_index\"]\n",
    "        ).single()\n",
    "        preview = rec.get(\"p\") if rec else \"\"\n",
    "        chunks.append(preview)\n",
    "driver.close()\n",
    "\n",
    "context = \"\\n\\n\".join(chunks)\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a friendly course assistant for a web development program.\n",
    "Answer the user's questions in a simple, smooth, natural way—like a mentor explaining.\n",
    "Do NOT mention chunks, retrieval, citations, or context.\n",
    "Use ONLY the provided context. If something is missing, say \"I’m not sure, the document didn’t mention that.\"\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "User question: {query}\n",
    "\n",
    "Course content context:\n",
    "{context}\n",
    "\n",
    "Now give a clear, straight-forward answer based ONLY on the above content.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nThinking...\\n\")\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=LLM_MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    "    temperature=0.3,\n",
    "    max_tokens=300\n",
    ")\n",
    "\n",
    "answer = resp.choices[0].message.content\n",
    "print(\"\\n--- Answer ---\\n\")\n",
    "print(answer)\n",
    "print(\"\\n--------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab95a3fd-b4f1-41bb-9cd3-5bedaee99a52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
